---
layout: user_guide
title: Pangool - User guide - Custom Partitioners
---
<div class="hero-unit">
	<h2>Pangool User Guide</h2>
</div>

<h2>Custom Partitioners</h2>

<p>
Usually the way Pangool partitions will be sufficient for almost all the cases. For Jobs that don’t use rollup, Pangool will partition by a combined hash of the group by fields. For information on how Rollup is handled, check the rollup section of the guide.
</p>

<p>
However, there are some cases where you’d want to partition by other fields (For instance, cases where you want to make your Job more parallelizable by shuffling the data and aggregating partial results in a second Job afterwards). In this cases you can use the convenience method:
</p>

<pre class="prettyprint" id="java">
	grouper.setCustomPartitionFields(fields)
</pre>

<p>
This method will use a combined hash of the fields that you’ll pass. These fields must be present in the intermediate schema.
</p>

<p>
However if you wanted to create a custom, other than combined hash strategy, you’ll need to implement your own org.apache.hadoop.mapreduce.Partitioner&lt;DatumWrapper&lt;ITuple&gt;, NullWritable&gt;  and assign it to the Job that you create out of the TupleMRBuilder. Check the current Partitioner implementation in the Pangool source for a reference implementation.
</p>

<p><a class="btn primary large" href="userguide8.html">Next: Custom Comparators &raquo;</a></p>