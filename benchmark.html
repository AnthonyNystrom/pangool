---
layout: user_guide
title: Pangool Benchmark
---
<div class="hero-unit">
	<h2>Pangool Benchmark</h2>
</div>

<p>
In order to demonstrate that Pangool performs comparable to Hadoop, we have designed a benchmark and executed it in EC2.
The machines used were of type m1.large. We used Cloudera's CDH3 with its default Hadoop configuration.
</p>
<p>
We have executed three different examples with three different input sizes. Each of the examples were launched in a comparable
implementation in Hadoop, Crunch and Cascading. You can see the implementations in the <a href='https://github.com/datasalt/pangool-benchmark'>pangool-benchmark project</a>. 
If you find something wrong, please send us your feedback. You are free to suggest alternative implementations that could perform better.
</p>
<p>
We have considered <a href='https://github.com/cloudera/crunch'>Crunch</a> and <a href='http://www.cascading.org/'>Cascading</a> for this benchmark in order to have more data points. 
They are, however, different APIs that were conceived with other requirements in mind. Pangool aims to be a replacament of the low-level MapReduce Hadoop Java API, whereas both Crunch and Cascading abstract the user from MapReduce - which allows them to implement easier flow management, something that Pangool doesn't (yet) aim for.
</p>
<p>
For Cascading, we used the 1.2.5 stable release. For Crunch, we built the 0.2.0 version in 2012-02 and used Avro serialization
because we found it to be much more efficient than TupleWritables.
</p> 
<p>
You'll find <a href='https://github.com/datasalt/pangool-benchmark/blob/master/launch-benchmark.sh'>a script in pangool-benchmark</a> that launches the full benchmark. Please read the header before launching it to
make sure you meet all the conditions for launching it.
</p>

<h2>Url Resolution</h2>

<p>
The <a href='https://github.com/datasalt/pangool/blob/master/examples/src/main/java/com/datasalt/pangool/examples/urlresolution/UrlResolution.java'>Url Resolution example in Pangool</a> shows how to perform a reduce-side join easily. We have implemented alternative versions of this example
in <a href='https://github.com/datasalt/pangool-benchmark/blob/master/src/main/java/com/datasalt/pangool/benchmark/urlresolution/HadoopUrlResolution.java'>Hadoop</a>, <a href='https://github.com/datasalt/pangool-benchmark/blob/master/src/main/java/com/datasalt/pangool/benchmark/urlresolution/CrunchUrlResolution.java'>Crunch</a> and <a href='https://github.com/datasalt/pangool-benchmark/blob/master/src/main/java/com/datasalt/pangool/benchmark/urlresolution/CascadingUrlResolution.java'>Cascading</a> (click to see each of them).
</p>
<p>
For generating the input datasets, we used the following commands (you'll find them in the launch-benchmark.sh script we mentioned above):
</p>
<pre>
	hadoop jar $PANGOOL_EXAMPLES_JAR url_resolution_gen_data url-map-1.txt url-reg-1.txt 1000 10 1000
	hadoop jar $PANGOOL_EXAMPLES_JAR url_resolution_gen_data url-map-2.txt url-reg-2.txt 5000 10 1000
	hadoop jar $PANGOOL_EXAMPLES_JAR url_resolution_gen_data url-map-3.txt url-reg-3.txt 10000 10 1000
</pre>
<p>
These are the results that we obtained in time (seconds) and IO (bytes):
</p>
<p>
<img src='images/benchmark/ur_t.png'/>
<img src='images/benchmark/ur_io.png'/>
</p>
<p>
We can see that all implementations had a comparable IO footprint. However, we clearly see that Pangool and Hadoop are much closer in efficiency than Crunch or Avro. In this case Pangool performed 8% worse than Hadoop, Crunch 27% and Cascading 65%.
</p>

<h2>Secondary sort</h2>

<p>
The <a href='https://github.com/datasalt/pangool/blob/master/examples/src/main/java/com/datasalt/pangool/examples/secondarysort/SecondarySort.java'>Secondary Sort example in Pangool</a> shows how to perform a secondary sort using multiple fields easily. We have implemented alternative versions of this
example in <a href='https://github.com/datasalt/pangool-benchmark/blob/master/src/main/java/com/datasalt/pangool/benchmark/secondarysort/HadoopSecondarySort.java'>Hadoop</a>, <a href='https://github.com/datasalt/pangool-benchmark/blob/master/src/main/java/com/datasalt/pangool/benchmark/secondarysort/CrunchSecondarySort.java'>Crunch</a> and <a href='https://github.com/datasalt/pangool-benchmark/blob/master/src/main/java/com/datasalt/pangool/benchmark/secondarysort/CascadingSecondarySort.java'>Cascading</a>.
</p>
<p>
For generating the input datasets, we used the following commands (you'll find them in the launch-benchmark.sh script we mentioned above):
</p>
<pre>
	hadoop jar $PANGOOL_BENCHMARK_JAR wordcount_gen_data wordcount-1.txt 1000000 50 4
	hadoop jar $PANGOOL_BENCHMARK_JAR wordcount_gen_data wordcount-2.txt 5000000 50 4
	hadoop jar $PANGOOL_BENCHMARK_JAR wordcount_gen_data wordcount-3.txt 10000000 50 4
</pre>
<p>
These are the results that we obtained in time (seconds) and IO (bytes):
</p>
<p>
<img src='images/benchmark/ss_t.png'/>
<img src='images/benchmark/ss_io.png'/>
</p>
<p>
In this example Hadoop and Pangool are both close in efficiency and IO footprint. However, Cascading deviates considerably in both aspects. Pangool performed 5% worse than Hadoop, Crunch 28% and Cascading 243%.
</p>

<h2>Standard Word Count</h2>

<p>
We used the default Word Count implementations of all APIs for this last benchmark. Because Word Count is a quite simple problem that doesn't involve joins or secondary sort, we are measuring here the most basic API overhead.
Click to see the implementations for <a href='https://github.com/datasalt/pangool-benchmark/blob/master/src/main/java/com/datasalt/pangool/benchmark/wordcount/PangoolWordCount.java'>Pangool</a>, <a href='https://github.com/datasalt/pangool-benchmark/blob/master/src/main/java/com/datasalt/pangool/benchmark/wordcount/HadoopWordCount.java'>Hadoop</a>, <a href='https://github.com/datasalt/pangool-benchmark/blob/master/src/main/java/com/datasalt/pangool/benchmark/wordcount/CrunchWordCount.java'>Crunch</a> and <a href='https://github.com/datasalt/pangool-benchmark/blob/master/src/main/java/com/datasalt/pangool/benchmark/wordcount/CascadingWordCount.java'>Cascading</a>.
</p>
<p>
For generating the input datasets, we used the following commands (you'll find them in the launch-benchmark.sh script we mentioned above):
</p>
<pre>
	hadoop jar $PANGOOL_BENCHMARK_JAR wordcount_gen_data wordcount-1.txt 1000000 50 4
	hadoop jar $PANGOOL_BENCHMARK_JAR wordcount_gen_data wordcount-2.txt 5000000 50 4
	hadoop jar $PANGOOL_BENCHMARK_JAR wordcount_gen_data wordcount-3.txt 10000000 50 4
</pre>
<p>
These are the results that we obtained in time (seconds) and IO (bytes):
</p>
<p>
<img src='images/benchmark/wc_t.png'/>
<img src='images/benchmark/wc_io.png'/>
</p>
<p>
We can see that in this case Hadoop, Pangool (5% worse) and Crunch (9% worse) performed similarly.
</p>
<p>
In this case, Cascading's overhead (257%) is not very meaningful because its default Word Count implementation doesn't include the use of its in-memory combiners whereas the other implementations (Pangool, Hadoop, Crunch) are already using Combiners.
<p>

<h1>Conclusions</h1>

<p>
We have seen how Pangool performs comparable to Hadoop in three different cases, performing between 5 and 8% worse 
according to this benchmark. Except for the word count example - where Pangool code is larger than Hadoop's word count 
becase we can't reuse the Combiner as we are not writing Tuples as output - the implementations in Pangool are much 
more concise and easy than those of Hadoop, which are extremely complex because of the use of custom Comparators, 
custom data types and other boilerplate code.
</p> 
<p>
Overall, in code conciseness, Cascading wins in all of the examples, but its performance is in other orders of magnitude. This is understandable, since Cascading is a higher-level API that solves a lot of problems that neither Hadoop or Pangool solve.
</p>
<p> 
On the other hand, we have found Crunch to perform very good when used in conjunction with Avro. It is an interesting API because it solves higher-level problems like flow management while retaining a decent performance. However, we found the resulting code for both the Url Resolution and Secondary Sort example to be extremely complex and hard to maintain. 
</p>  